_target_: src.models.stylegan_module.StyleGANModule
_recursive_: False
hparams:
  z_dim: &z_dim 512
  netG:
    _target_: src.models.components.generators.stylegan.Generator
    c_dim: 0
    img_resolution: 256
    img_channels: 3
    z_dim: *z_dim
    w_dim: 512
    mapping_kwargs:
      num_layers: 8
    synthesis_kwargs:
      channel_base: 16384
      channel_max: 512
      num_fp16_res: 4
      conv_clamp: 256

  norm:
    mean: ${datamodule.transform_params.mean}
    std: ${datamodule.transform_params.std}

  # Train specific parameters
  train:

    netD: # Discriminator model
      _target_: src.models.components.discriminators.stylegan_discriminator.Discriminator
      c_dim: 0
      img_resolution: 256
      img_channels: 3
      epilogue_kwargs:
        mbstd_group_size: 8
      channel_base: 16384
      channel_max: 512
      num_fp16_res: 4
      conv_clamp: 256

    initialization:
      pretrain_checkpoint_G: ${pretrain_gen}  # If passed model loads model else init_G argument initialization
      init_G: # Initialization params for generator
        init_type: null
        init_gain: 0.02
      init_D: # Initialization params for discriminator
        init_type: null
        init_gain: 0.02

    speed:
      cudnn_benchmark: False
      allow_tf32: False

    style_mixing_prob: 0.9

    ada_augs:
      name: bgc

      ada_target: 0.6
      ada_gamma: 0.99
      ada_interval: 8
      ada_kimg: 100


    ema:
      ema_kimg: 20
      ema_rampup: null


    losses:
      r1:
        r1_gamma: 0.8192
        D_reg_interval: 16

      path_length:
        pl_batch_shrink: 2
        pl_decay: 0.01
        pl_weight: 2
        G_reg_interval: 4


    logging:
      img_log_freq: 400



    optimizing:
      optimizers:
        optimizer_G:
          _target_: torch.optim.Adam
          lr: &lr_G 0.002
          betas: [ 0.0, 0.9919919678228657 ]

        optimizer_D:
          _target_: torch.optim.Adam
          lr: &lr_D 0.0023529411764705885
          betas: [ 0.0, 0.9905854573074332 ]
      schedulers:
        interval: 'step'
        scheduler_G:
          _target_: torch.optim.lr_scheduler.OneCycleLR
          max_lr: *lr_G
          epochs: ${trainer.max_epochs}
          steps_per_epoch: ${trainer.limit_train_batches}
          pct_start: ${onecycle_warmup_epoches:${trainer.max_epochs}, 1}  # The percentage of the cycle (in number of steps) spent increasing the learning rate.
          div_factor: 25  # Determines the initial learning rate via initial_lr = max_lr/div_factor Default: 25
          final_div_factor: ${get_one_cycle_final_div_factor:${model.hparams.train.optimizing.schedulers.scheduler_G.div_factor}, 1}  # Determines the minimum learning rate via min_lr = initial_lr/final_div_factor
          cycle_momentum: False # !!!!Affect adam betas if True
        scheduler_D:
          _target_: torch.optim.lr_scheduler.OneCycleLR
          max_lr: *lr_D
          epochs: ${trainer.max_epochs}
          steps_per_epoch: ${trainer.limit_train_batches}
          pct_start: ${onecycle_warmup_epoches:${trainer.max_epochs}, 1}  # The percentage of the cycle (in number of steps) spent increasing the learning rate.
          div_factor: 25  # Determines the initial learning rate via initial_lr = max_lr/div_factor Default: 25
          final_div_factor: ${get_one_cycle_final_div_factor:${model.hparams.train.optimizing.schedulers.scheduler_G.div_factor}, 1}  # Determines the minimum learning rate via min_lr = initial_lr/final_div_factor
          cycle_momentum: False # !!!!Affect adam betas if True

  val:
    val_path: ${paths.data_dir}ffhq/train_subset
    num_workers: ${datamodule.num_workers}

